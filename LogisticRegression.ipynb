{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Synthetic Data\n",
    "\n",
    "For more explanation of logistic regression, see\n",
    "1. [Our course notes](https://jennselby.github.io/MachineLearningCourseNotes/#binomial-logistic-regression)\n",
    "1. [This scikit-learn explanation](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "1. [The full scikit-learn documentation of the LogisticRegression model class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random # for generating our dataset\n",
    "import math\n",
    "from sklearn import linear_model # for fitting our model\n",
    "\n",
    "# force numpy not to use scientific notation, to make it easier to read the numbers the program prints out\n",
    "numpy.set_printoptions(suppress=True)\n",
    "\n",
    "# to display graphs in this notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "As we did in the [linear regression notebook](https://nbviewer.jupyter.org/github/jennselby/MachineLearningCourseNotes/blob/master/assets/ipynb/LinearRegression.ipynb), we will be generating some fake data.\n",
    "\n",
    "In this fake dataset, we have two types of plants.\n",
    "* Plant A tends to be taller (average 60cm) and thinner (average 8cm).\n",
    "* Plant B tends to be shorter (average 58cm) and wider (average 10cm).\n",
    "* The heights and diameters of both plants are normally distributed (they follow a bell curve).\n",
    "\n",
    "* Class 0 will represent Plant A and Class 1 will represent Plant B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "NUM_INPUTS = 50 # inputs per class\n",
    "PLANT_A_AVG_HEIGHT = 70.0\n",
    "PLANT_A_AVG_WIDTH = 8.0\n",
    "PLANT_B_AVG_HEIGHT = 58.0\n",
    "PLANT_B_AVG_WIDTH = 10.0\n",
    "\n",
    "# Pick numbers randomly with a normal distribution centered around the averages\n",
    "\n",
    "plant_a_heights = numpy.random.normal(loc=PLANT_A_AVG_HEIGHT, size=NUM_INPUTS)\n",
    "plant_a_widths = numpy.random.normal(loc=PLANT_A_AVG_WIDTH, size=NUM_INPUTS)\n",
    "\n",
    "plant_b_heights = numpy.random.normal(loc=PLANT_B_AVG_HEIGHT, size=NUM_INPUTS)\n",
    "plant_b_widths = numpy.random.normal(loc=PLANT_B_AVG_WIDTH, size=NUM_INPUTS)\n",
    "\n",
    "# this creates a 2-dimensional matrix, with heights in the first column and widths in the second\n",
    "# the first half of rows are all plants of type a and the second half are type b\n",
    "plant_inputs = list(zip(numpy.append(plant_a_heights, plant_b_heights),\n",
    "                        numpy.append(plant_a_widths, plant_b_widths)))\n",
    "\n",
    "# this is a list where the first half are 0s (representing plants of type a) and the second half are 1s (type b)\n",
    "classes = [0]*NUM_INPUTS + [1]*NUM_INPUTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Let's visualize our dataset, so that we can better understand what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a figure and label it\n",
    "fig = matplotlib.pyplot.figure()\n",
    "fig.suptitle('Plant Data Set')\n",
    "matplotlib.pyplot.xlabel('Height')\n",
    "matplotlib.pyplot.ylabel('Width')\n",
    "\n",
    "# put the generated points on the graph\n",
    "a_scatter = matplotlib.pyplot.scatter(plant_a_heights, plant_a_widths, c=\"red\", marker=\"o\", label='plant a')\n",
    "b_scatter = matplotlib.pyplot.scatter(plant_b_heights, plant_b_widths, c=\"blue\", marker=\"^\", label='plant b')\n",
    "\n",
    "# add a legend to explain which points are which\n",
    "matplotlib.pyplot.legend(handles=[a_scatter, b_scatter])\n",
    "\n",
    "# show the graph\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Next, we want to fit our logistic regression model to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(69.83306712389773, 9.768535189283249), (71.96753543191161, 8.881715507282392), (69.19938981290863, 7.002704296490986), (70.14911696638445, 9.933178869329314), (70.13989745743498, 10.143361640452225), (70.59719150439606, 9.595052894811642), (70.39659677357315, 6.671640139318971), (71.38518726492369, 7.31712765606842), (69.44159395998562, 10.32153214598432), (70.51032162658713, 8.574128575786636), (71.46508342944699, 7.764980673251516), (69.56229095873717, 8.496576717645741), (69.99808363213168, 8.828977016513404), (69.328896310628, 7.653155377708035), (70.53680820259753, 7.778006593772981), (70.85841647355124, 8.954599410143896), (69.89530324762049, 8.380803307862319), (69.8640888155109, 6.690468797387773), (69.3971439505749, 7.857760218625558), (71.3916253753151, 8.750334754159523), (69.14459435577898, 7.745640478436013), (71.22781177514611, 7.904509152653887), (68.85676913416486, 8.803396785740613), (71.57791375996499, 8.427544695325032), (69.24553197431115, 8.050457315330547), (69.3534907466395, 8.644949366465706), (68.74838341279909, 7.072749391539483), (69.93953413276559, 9.06971798851866), (69.87331257239305, 9.56397957761979), (68.92438782703505, 8.705184783715064), (69.67456204738099, 8.017997618114379), (71.0242774779161, 5.922712333588267), (70.03220397053008, 8.326851937728613), (71.65327757259371, 9.71398755431534), (69.9713569809448, 7.674111635416037), (68.2584663669819, 8.423606760816261), (69.461387289612, 8.310696482075018), (71.10253813009345, 6.77840815473275), (68.66462022602032, 8.149404997656218), (70.2449899208856, 5.728515251298138), (70.53546870612658, 7.953708506869685), (70.16486128180989, 8.322911383201175), (70.30985137167036, 8.267147733365759), (70.50582224277039, 9.020408210520213), (68.35092025932131, 8.931833574963319), (69.09949406429462, 8.920302312799691), (70.8832528102493, 9.002565794535865), (70.55682219334595, 8.31053776164921), (70.1579394328198, 8.961589845776867), (68.33147117115129, 7.249120589574544), (58.231690658535655, 10.269533668293409), (58.62131255898078, 10.569607982411227), (57.992320554618416, 9.804589924123503), (57.71788077723731, 9.05797963153596), (59.847192244385646, 10.937140880951276), (57.07010923277032, 10.034289469653988), (55.94137057981534, 10.026237822366648), (58.3571542007376, 9.743000411117858), (57.914456806099174, 11.874070814404188), (58.769894710593064, 10.158310024027612), (58.49127716933123, 7.958464798359074), (59.66658691543582, 9.40433352870409), (59.15173142321177, 9.652748919616457), (58.1591102692489, 9.935801052371804), (57.94249705830663, 9.554168758141131), (59.39666404445754, 8.465461552012222), (58.367435457349295, 10.903237092489064), (58.398618067645074, 9.785515893478596), (57.26656406808599, 8.967341814280621), (57.12973159080884, 7.893494942723293), (56.24933805391516, 11.461853946653951), (59.22029389870257, 10.102017189245357), (56.671878636939546, 11.306746500518019), (58.87755144534524, 9.835204634134314), (58.06923878069856, 11.05106354589652), (58.068144252538026, 8.61465097911396), (58.752104228702194, 9.791131104899618), (57.048440095815856, 10.129921676659514), (60.34030914953904, 10.954169710953694), (57.68253296241034, 10.094255088681797), (56.5865864866808, 11.814654511691812), (58.36654513731445, 10.951734893755903), (57.28404144047179, 10.185441904003612), (58.691485633431824, 9.414474919186858), (57.56543822783752, 8.113696415462597), (58.362668211069256, 10.626732951256342), (59.32101147801161, 10.529244860367209), (58.320873989232396, 10.394474561715759), (59.11206825966076, 11.97253517206438), (58.01176062706449, 8.615596885588685), (58.81245199725154, 10.784532758338662), (58.29718129476868, 10.540257621062878), (57.93769812742257, 10.054596853030908), (59.82238641754927, 10.059631486312213), (57.770500393736356, 10.273514793279361), (57.71376737666156, 10.81739096091058), (58.054603049213334, 9.718511862753914), (58.442133099856385, 9.71480211912985), (59.9564453737331, 10.474387081195884), (56.16193355684939, 8.324877160663931)]\n",
      "Intercept: [0.75349519]  Coefficients: [[-0.35137778  2.36847399]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(plant_inputs, classes)\n",
    "print('Intercept: {0}  Coefficients: {1}'.format(model.intercept_, model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Now we can make some predictions using the trained model. Note that we are generating the new data exactly the same way that we generated the training data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant A: 70.17320182571817 6.085554396970032\n",
      "Plant B: 57.73548706587579 10.703336031017077\n",
      "Class predictions: [0 1]\n",
      "Probabilities:\n",
      "[[0.99992443 0.00007557]\n",
      " [0.00296739 0.99703261]]\n"
     ]
    }
   ],
   "source": [
    "# Generate some new random values for two plants, one of each class\n",
    "new_a_height = numpy.random.normal(loc=PLANT_A_AVG_HEIGHT)\n",
    "new_a_width = numpy.random.normal(loc=PLANT_A_AVG_WIDTH)\n",
    "new_b_height = numpy.random.normal(loc=PLANT_B_AVG_HEIGHT)\n",
    "new_b_width = numpy.random.normal(loc=PLANT_B_AVG_WIDTH)\n",
    "\n",
    "# Pull the values into a matrix, because that is what the predict function wants\n",
    "inputs = [[new_a_height, new_a_width], [new_b_height, new_b_width]]\n",
    "\n",
    "# Print out the outputs for these new inputs\n",
    "print('Plant A: {0} {1}'.format(new_a_height, new_a_width))\n",
    "print('Plant B: {0} {1}'.format(new_b_height, new_b_width))\n",
    "print('Class predictions: {0}'.format(model.predict(inputs))) # guess which class\n",
    "print('Probabilities:\\n{0}'.format(model.predict_proba(inputs))) # give probability of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option (Standard Difficulty)\n",
    "\n",
    "Answer the following questions. You can also use the graph below, if seeing the data visually helps you understand the data.\n",
    "1. What should we be expecting as the output for class predictions in the above cell? If the model is not giving the expected output, what are some of the reasons it might not be?\n",
    "1. How do the probabilities output by the above cell relate to the class predictions? Why do you think the model might be more or less confident in its predictions?\n",
    "1. If you change the averages in the data generation code (like PLANT_A_AVG_HEIGHT) and re-run the code, how do the predictions change, and why?\n",
    "1. Looking at the intercept and coefficient output further above, if a coefficient is negative, what has the model learned about this feature? In other words, if you took a datapoint and you increased the value of a feature that has a negative coefficient, what would you expect to happen to the probabilities the model gives this datapoint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We should be expecting 0 and 1. If we don't get the expected output, the model might be underfitting. \n",
    "2. For making predictions, the model will select the one with the highest probability. All plants have traits that slightly match both models.\n",
    "3. If you make the averages closer, the probability will decrease, while if you make the averages further apart, the the probability will increase.\n",
    "4. If a coefficient is negative, that means Class 1 will be smaller in that feature than Class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option (Advanced)\n",
    "\n",
    "The plot above is only showing the data, and not anything about what the model learned. Come up with some ideas for how to show the model fit and implement one of them in code. Remember, we are here to help if you are not sure how to write the code for your ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n"
     ]
    }
   ],
   "source": [
    "# from scipy.special import expit\n",
    "\n",
    "# figModel = matplotlib.pyplot.figure()\n",
    "# figModel.suptitle('Plant Data Set')\n",
    "# matplotlib.pyplot.xlabel('Height')\n",
    "# matplotlib.pyplot.ylabel('Width')\n",
    "\n",
    "# # put the generated points on the graph\n",
    "# a_scatter = matplotlib.pyplot.scatter(plant_a_heights, plant_a_widths, c=\"red\", marker=\"o\", label='plant a')\n",
    "# b_scatter = matplotlib.pyplot.scatter(plant_b_heights, plant_b_widths, c=\"blue\", marker=\"^\", label='plant b')\n",
    "\n",
    "# # add a legend to explain which points are which\n",
    "# matplotlib.pyplot.legend(handles=[a_scatter, b_scatter])\n",
    "# all_heights = plant_a_heights+plant_b_heights\n",
    "# all_widths = plant_a_widths + plant_b_widths\n",
    "# plant_min_height = math.floor(min(all_heights))\n",
    "# plant_max_height = math.ceil(max(all_heights))\n",
    "# plant_min_width = math.floor(min(all_widths))\n",
    "# plant_max_width = math.ceil(max(all_widths))\n",
    "# Xheight = numpy.linspace(plant_min_height, plant_max_height)\n",
    "# Xwidth = numpy.linspace(plant_min_width, plant_max_width)\n",
    "# X = numpy.array([Xheight, Xwidth])\n",
    "# X_transpose = X.transpose()\n",
    "# X2 = list(zip(X_transpose))\n",
    "# print(X2)\n",
    "# coef_tranpose = model.coef_.transpose()\n",
    "# print(X2 * model.coef_ + model.intercept_)\n",
    "# Y = model.predict(X2)\n",
    "# print(Y)\n",
    "# matplotlib.pyplot.plot(X, Y)\n",
    "\n",
    "# # show the graph\n",
    "# matplotlib.pyplot.show()\n",
    "print(model.score(plant_inputs, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option (Advanced)\n",
    "\n",
    "If you have more than two classes, you can use multinomial logistic regression or the one vs. rest technique, where you use a binomial logistic regression for each class that you have and decide if it is or is not in that class. Try expanding the program with a third type and implementing your own one vs. rest models. To test if this is working, compare your output to running your expanded dataset through scikit-learn, which will automatically do one vs. rest if there are more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[80  9]]\n",
      "0.33983875171791444\n",
      "0.0492224558415585\n",
      "0.5138366743989241\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00167359, 0.        , 0.99832641]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLANT_C_AVG_HEIGHT = 80.0\n",
    "PLANT_C_AVG_WIDTH = 9.0\n",
    "plant_c_heights = numpy.random.normal(loc=PLANT_C_AVG_HEIGHT, size=NUM_INPUTS)\n",
    "plant_c_widths = numpy.random.normal(loc=PLANT_C_AVG_WIDTH, size=NUM_INPUTS)\n",
    "\n",
    "plant_inputs_extra = list(zip(numpy.append((plant_a_heights, plant_b_heights), plant_c_heights),\n",
    "                        numpy.append((plant_a_widths, plant_b_widths), plant_c_widths)))\n",
    "#print(plant_inputs_extra)\n",
    "classes_extra = [0]*(NUM_INPUTS*2) + [1]*NUM_INPUTS\n",
    "test_input = numpy.array((80,9)).reshape(1,-1)\n",
    "def Triple_Logistic(a_heights, a_widths, b_heights, b_widths, c_heights, c_widths):\n",
    "    inputs = list(zip(numpy.append((a_heights, a_heights), c_heights),\n",
    "                        numpy.append((a_widths, b_widths), c_widths)))\n",
    "    classes_1 = [1]*NUM_INPUTS + [0]*NUM_INPUTS + [0]*NUM_INPUTS \n",
    "    classes_2 = [0]*NUM_INPUTS + [1]*NUM_INPUTS + [0]*NUM_INPUTS \n",
    "    classes_3 = [0]*NUM_INPUTS + [0]*NUM_INPUTS + [1]*NUM_INPUTS \n",
    "    model1 = linear_model.LogisticRegression()\n",
    "    model1.fit(inputs, classes_1)\n",
    "    model2 = linear_model.LogisticRegression()\n",
    "    model2.fit(inputs, classes_2)\n",
    "    model3 = linear_model.LogisticRegression()\n",
    "    model3.fit(inputs, classes_3)\n",
    "    print(test_input)\n",
    "    def Triple_Predict(plant_input, model1, model2, model3):\n",
    "        prob1 = model1.predict_proba(plant_input)[0][1]\n",
    "        prob2 = model2.predict_proba(plant_input)[0][1]\n",
    "        prob3 = model3.predict_proba(plant_input)[0][1]\n",
    "        print(prob1)\n",
    "        print(prob2)\n",
    "        print(prob3)\n",
    "        if prob1 >= prob2 and prob1 >= prob3:\n",
    "            return 1\n",
    "        elif prob2 >= prob1 and prob2 >= prob3:\n",
    "            return 2\n",
    "        return 3\n",
    "    \n",
    "    print(Triple_Predict(test_input, model1, model2, model3))\n",
    "Triple_Logistic(plant_a_heights, plant_a_widths, plant_b_heights, plant_b_widths, plant_c_heights, plant_c_widths)\n",
    "model_super = linear_model.LogisticRegression(multi_class='multinomial', solver ='lbfgs')\n",
    "classes_4 = [0]*NUM_INPUTS + [1]*NUM_INPUTS + [2]*NUM_INPUTS \n",
    "model_super.fit(plant_inputs_extra, classes_4)\n",
    "model_super.predict_proba(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
